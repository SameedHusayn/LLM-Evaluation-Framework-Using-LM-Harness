{
  "results": {
    "tinyBenchmarks": {
      " ": " ",
      "alias": "tinyBenchmarks"
    },
    "tinyArc": {
      "alias": " - tinyArc",
      "acc_norm,none": 0.07477248802048313,
      "acc_norm_stderr,none": "N/A"
    },
    "tinyGSM8k": {
      "alias": " - tinyGSM8k",
      "exact_match,strict-match": 0.005529795100002627,
      "exact_match_stderr,strict-match": "N/A",
      "exact_match,flexible-extract": 0.005529795100002627,
      "exact_match_stderr,flexible-extract": "N/A"
    },
    "tinyHellaswag": {
      "alias": " - tinyHellaswag",
      "acc_norm,none": 0.02532334655713725,
      "acc_norm_stderr,none": "N/A"
    },
    "tinyMMLU": {
      "alias": " - tinyMMLU",
      "acc_norm,none": 0.12943396745485464,
      "acc_norm_stderr,none": "N/A"
    },
    "tinyTruthfulQA": {
      "alias": " - tinyTruthfulQA",
      "acc,none": 0.02554442717564341,
      "acc_stderr,none": "N/A"
    },
    "tinyWinogrande": {
      "alias": " - tinyWinogrande",
      "acc_norm,none": 0.10750900653611216,
      "acc_norm_stderr,none": "N/A"
    }
  },
  "group_subtasks": {
    "tinyBenchmarks": [
      "tinyArc",
      "tinyGSM8k",
      "tinyMMLU",
      "tinyWinogrande",
      "tinyHellaswag",
      "tinyTruthfulQA"
    ]
  },
  "configs": {
    "tinyArc": {
      "task": "tinyArc",
      "dataset_path": "tinyBenchmarks/tinyAI2_arc",
      "dataset_name": "ARC-Challenge",
      "training_split": "train",
      "validation_split": "validation",
      "test_split": "test",
      "doc_to_text": "Question: {{question}}\nAnswer:",
      "doc_to_target": "{{choices.label.index(answerKey)}}",
      "doc_to_choice": "{{choices.text}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 25,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "def agg_gpirt_arc(items: List[float], benchmark: str = \"arc\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "Question: {{question}}\nAnswer:",
      "metadata": {
        "version": 0.0
      }
    },
    "tinyGSM8k": {
      "task": "tinyGSM8k",
      "dataset_path": "tinyBenchmarks/tinyGSM8k",
      "dataset_name": "main",
      "training_split": "train",
      "test_split": "test",
      "fewshot_split": "train",
      "doc_to_text": "Question: {{question}}\nAnswer:",
      "doc_to_target": "{{answer}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "def agg_gpirt_gsm8k(items: List[float], benchmark: str = \"gsm8k\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": false,
          "regexes_to_ignore": [
            ",",
            "\\$",
            "(?s).*#### ",
            "\\.$"
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Question:",
          "</s>",
          "<|im_end|>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "strict-match",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "#### (\\-?[0-9\\.\\,]+)"
            },
            {
              "function": "take_first"
            }
          ]
        },
        {
          "name": "flexible-extract",
          "filter": [
            {
              "function": "regex",
              "group_select": -1,
              "regex_pattern": "(-?[$0-9.,]{2,})|(-?[0-9]+)"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "tinyHellaswag": {
      "task": "tinyHellaswag",
      "dataset_path": "tinyBenchmarks/tinyHellaswag",
      "training_split": "train",
      "validation_split": "validation",
      "process_docs": "def process_docs(dataset: datasets.Dataset) -> datasets.Dataset:\n    def _process_doc(doc):\n        ctx = doc[\"ctx_a\"] + \" \" + doc[\"ctx_b\"].capitalize()\n        out_doc = {\n            \"query\": preprocess(doc[\"activity_label\"] + \": \" + ctx),\n            \"choices\": [preprocess(ending) for ending in doc[\"endings\"]],\n            \"gold\": int(doc[\"label\"]),\n        }\n        return out_doc\n\n    return dataset.map(_process_doc)\n",
      "doc_to_text": "{{query}}",
      "doc_to_target": "{{label}}",
      "doc_to_choice": "choices",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 10,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "def agg_gpirt_hellaswag(items: List[float], benchmark: str = \"hellaswag\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "tinyMMLU": {
      "task": "tinyMMLU",
      "dataset_path": "tinyBenchmarks/tinyMMLU",
      "dataset_name": "all",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{input_formatted}}",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "A",
        "B",
        "C",
        "D"
      ],
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "fewshot_config": {
        "sampler": "first_n"
      },
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "def agg_gpirt_mmlu(items: List[float], benchmark: str = \"mmlu\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "tinyTruthfulQA": {
      "task": "tinyTruthfulQA",
      "dataset_path": "tinyBenchmarks/tinyTruthfulQA",
      "dataset_name": "multiple_choice",
      "validation_split": "validation",
      "doc_to_text": "{% set prompt_qa = 'Q: What is human life expectancy in the United States?\nA: Human life expectancy in the United States is 78 years.\n\nQ: Who was president of the United States in 1955?\nA: Dwight D. Eisenhower was president of the United States in 1955.\n\nQ: Which party did he belong to?\nA: He belonged to the Republican Party.\n\nQ: What is the square root of banana?\nA: I have no comment.\n\nQ: How does a telescope work?\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n\nQ: Where were the 1992 Olympics held?\nA: The 1992 Olympics were held in Barcelona, Spain.'%}{{prompt_qa + '\n\nQ: ' + question + '\nA:'}}",
      "doc_to_target": 0,
      "doc_to_choice": "{{mc2_targets.choices}}",
      "process_results": "def process_results_mc2(doc, results):\n    lls, is_greedy = zip(*results)\n\n    # Split on the first `0` as everything before it is true (`1`).\n    split_idx = list(doc[\"mc2_targets\"][\"labels\"]).index(0)\n    # Compute the normalized probability mass for the correct answer.\n    ll_true, ll_false = lls[:split_idx], lls[split_idx:]\n    p_true, p_false = np.exp(np.array(ll_true)), np.exp(np.array(ll_false))\n    p_true = p_true / (sum(p_true) + sum(p_false))\n\n    return {\"acc\": sum(p_true)}\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "def agg_gpirt_truthfulqa(items: List[float], benchmark: str = \"truthfulqa\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "question",
      "metadata": {
        "version": 0.0
      }
    },
    "tinyWinogrande": {
      "task": "tinyWinogrande",
      "dataset_path": "tinyBenchmarks/tinyWinogrande",
      "dataset_name": "winogrande_xl",
      "training_split": "train",
      "validation_split": "validation",
      "doc_to_text": "def doc_to_text(doc):\n    answer_to_num = {\"1\": 0, \"2\": 1}\n    return answer_to_num[doc[\"answer\"]]\n",
      "doc_to_target": "def doc_to_target(doc):\n    idx = doc[\"sentence\"].index(\"_\") + 1\n    return doc[\"sentence\"][idx:].strip()\n",
      "doc_to_choice": "def doc_to_choice(doc):\n    idx = doc[\"sentence\"].index(\"_\")\n    options = [doc[\"option1\"], doc[\"option2\"]]\n    return [doc[\"sentence\"][:idx] + opt for opt in options]\n",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "acc_norm",
          "aggregation": "def agg_gpirt_winogrande(items: List[float], benchmark: str = \"winogrande\") -> float:\n    items = np.array(items)\n    predictions = tb.evaluate(items, benchmark)\n    return predictions[benchmark][\"gpirt\"]\n",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": true,
      "doc_to_decontamination_query": "sentence",
      "metadata": {
        "version": 0.0
      }
    }
  },
  "versions": {
    "tinyArc": 0.0,
    "tinyGSM8k": 0.0,
    "tinyHellaswag": 0.0,
    "tinyMMLU": 0.0,
    "tinyTruthfulQA": 0.0,
    "tinyWinogrande": 0.0
  },
  "n-shot": {
    "tinyArc": 25,
    "tinyGSM8k": 5,
    "tinyHellaswag": 10,
    "tinyMMLU": 0,
    "tinyTruthfulQA": 0,
    "tinyWinogrande": 5
  },
  "higher_is_better": {
    "tinyArc": {
      "acc_norm": true
    },
    "tinyBenchmarks": {
      "acc_norm": true,
      "exact_match": true,
      "acc": true
    },
    "tinyGSM8k": {
      "exact_match": true
    },
    "tinyHellaswag": {
      "acc_norm": true
    },
    "tinyMMLU": {
      "acc_norm": true
    },
    "tinyTruthfulQA": {
      "acc": true
    },
    "tinyWinogrande": {
      "acc_norm": true
    }
  },
  "n-samples": {
    "tinyArc": {
      "original": 100,
      "effective": 1
    },
    "tinyGSM8k": {
      "original": 100,
      "effective": 1
    },
    "tinyMMLU": {
      "original": 100,
      "effective": 1
    },
    "tinyWinogrande": {
      "original": 100,
      "effective": 1
    },
    "tinyHellaswag": {
      "original": 100,
      "effective": 1
    },
    "tinyTruthfulQA": {
      "original": 100,
      "effective": 1
    }
  },
  "config": {
    "model": "openai-completions",
    "model_args": "model=davinci-002,trust_remote_code=True",
    "batch_size": 1,
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": 1,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "69226f5d",
  "date": 1727344130.2513013,
  "pretty_env_info": "PyTorch version: 2.3.1+cpu\nIs debug build: False\nCUDA used to build PyTorch: None\nROCM used to build PyTorch: N/A\n\nOS: Microsoft Windows 11 Pro\nGCC version: Could not collect\nClang version: Could not collect\nCMake version: version 3.30.2\nLibc version: N/A\n\nPython version: 3.12.2 (tags/v3.12.2:6abddd9, Feb  6 2024, 21:26:36) [MSC v.1937 64 bit (AMD64)] (64-bit runtime)\nPython platform: Windows-11-10.0.22631-SP0\nIs CUDA available: False\nCUDA runtime version: No CUDA\nCUDA_MODULE_LOADING set to: N/A\nGPU models and configuration: No CUDA\nNvidia driver version: No CUDA\ncuDNN version: No CUDA\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture=9\r\r\nCurrentClockSpeed=2208\r\r\nDeviceID=CPU0\r\r\nFamily=206\r\r\nL2CacheSize=512\r\r\nL2CacheSpeed=\r\r\nManufacturer=GenuineIntel\r\r\nMaxClockSpeed=2208\r\r\nName=Intel(R) Core(TM) i3-8130U CPU @ 2.20GHz\r\r\nProcessorType=3\r\r\nRevision=\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.3.1\n[conda] Could not collect",
  "transformers_version": "4.41.2",
  "upper_git_hash": null,
  "task_hashes": {
    "tinyArc": "e13f6335fe3c783cb156e63febdd136949359e2c63a6f3b67b0a5151419042f2",
    "tinyGSM8k": "1ced083791a416dc5e3dd59daa55422df0784f963fe2f7db3d796ecb27db7a2d",
    "tinyMMLU": "a8d04b1e40e5ff6f31d154c5c878dc9d8c806e37b9037b5cf349e8eb887d168f",
    "tinyWinogrande": "ffdabae032a78b21d7f84be31ad3867387aa3834863cd042c670270d9ee3041e",
    "tinyHellaswag": "1c527e67b503b914d7b2cee9bb1af7d5c49c3b32ec999f6e87f80c96f2c17b83",
    "tinyTruthfulQA": "ea25df0b1626f607f186fce13be04ab4ee03f94f574968dd7d6bb69eaf524715"
  },
  "model_source": "openai-completions",
  "model_name": "davinci-002",
  "model_name_sanitized": "davinci-002",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": "",
  "chat_template_sha": null,
  "start_time": 168103.5769537,
  "end_time": 168206.8806601,
  "total_evaluation_time_seconds": "103.3037064000091"
}